# Mind the Gap! Bridging Explainable Artificial Intelligence and Human Understanding #

Myriad approaches exist to help humans peer inside automated decision-making
systems based on artificial intelligence and machine learning algorithms.
These tools and the insights they produce, however, tend to be complex
socio-technological constructs themselves, hence subject to technical
limitations as well as human biases and (possibly ill-defined) preferences.
Under these conditions, how can we ensure that explanations are meaningful and
fulfil their role by leading to understanding?

In this talk I will provide a high-level introduction to and overview of
explainable AI and interpretable ML, followed by a deep dive into practical
aspects of a popular explainability algorithm.
I will demonstrate how different configurations of an explainer -- that is
often presented as a monolithic, end-to-end tool -- may impact the resulting
insights.
I will then show the importance of the strategy employed to present them
to a user, arguing in favour of a clear separation between the technical and
social aspects of such techniques.
Importantly, understanding these dependencies can help us to build bespoke
explainers that are robust, reliable, trustworthy and suitable for the unique
problem at hand.

> Interactive slides for an invited talk given at
> *ETH Z&uuml;rich*, Switzerland.

This directory holds [slides](slides) used for the presentation.
See <https://events.fat-forensics.org/2022_eth/> for more details.
