---
layout: custom
permalink: /2021_bias

description: A 2021 Bristol Interactive AI Summer School (BIAS) session with a hands-on component

show_resources: true
notebooks: https://github.com/fat-forensics/resources/tree/master/tabular_surrogate_builder/
slides: https://github.com/fat-forensics/events/tree/master/resources/2021_BIAS-summer-school/slides/
demo: https://github.com/fat-forensics/resources/tree/master/surrogates_overview/
---

# Practical Machine Learning Explainability #
{:.no_toc}
## Surrogate Explainers and Fairwashing ##
{:.no_toc}

<table>
  <thead>
    <tr>
      <th style="text-align: left" colspan="2">A hands-on session at the <a href="https://www.bristol.ac.uk/cdt/interactive-ai/events/bias-summer-school/">2021 Bristol Interactive AI Summer School (BIAS)</a>.</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Where:</td>
      <td style="text-align: left">The Bill Brown Suite, Queens Building, University of Bristol, Bristol, United Kingdom.</td>
    </tr>
    <tr>
      <td style="text-align: left">When:</td>
      <td style="text-align: left">Monday, September 6<sup>th</sup>, 2021.</td>
    </tr>
  </tbody>
</table>

## Table of Contents ##
{:.no_toc}

* TOC
{:toc}

## About the Session ##
Surrogate explainability is a popular transparency technique for assessing
trustworthiness of predictions output by black-box machine learning models.
While such explainers are often presented as monolithic, end-to-end tools,
they in fact exhibit high modularity and scope for parameterisation[^2].
This observation suggests that each use case may require a bespoke surrogate
built and tuned for the problem at hand.
This session introduces the three core components of surrogate explainers:
data sampling, interpretable representation and explanation generation
in view of text, image and tabular data.
By understanding these building blocks individually, as well as their interplay,
we can build robust and trustworthy explainers.
However, we can also misuse these insights to create technically-valid
explainers that are intended to produce misleading justifications of
individual predictions.
For example, by manipulating the size and distribution of the data sample, and
the grouping criteria of the interpretable representation, an automated decision
may be shown as fair despite the underlying model being inherently biased.
This overview of theory is complemented by a **no-code** hands-on exercise
facilitated through an *iPython widget* delivered via a Jupyter Notebook.

## FAT Forensics (Software) ##
To support the goals of this session, we employ
[`FAT Forensics`](https://fat-forensics.org/) -- an open source Python package
that can inspect selected fairness, accountability and **transparency** aspects
of data (and their features), *models* and *predictions*.
The toolbox spans all of the FAT domains because many of them share underlying
algorithmic components that can be reused in multiple different
implementations, often across the FAT borders.
This *interoperability* allows, for example, a counterfactual data point
generator to be used as a post-hoc explainer of black-box predictions on
one hand, and as an individual fairness (disparate treatment) inspection tool
on the other.
The *modular* architecture[^1][^3] enables
[`FAT Forensics`](https://fat-forensics.org/) to deliver robust and tested
low-level FAT building blocks as well as a collection of FAT tools built on top
of them.
Users can choose from these ready-made tools or, alternatively, combine the
available building blocks to create their own bespoke algorithms without the
need of modifying the code base.

## Schedule and Resources ##
The session lasts for 2 hours.
The first part -- 1 hour and 15 minutes -- introduces surrogate explainers of
text, image and tabular data, discussing their pros, cons and modularisation.
The next part -- 45 minutes -- is devoted to a hands-on exercise demonstrating
the importance of tabular surrogate parameterisation, data sampling and
interpretable representation composition (discretisation of numerical features)
in particular.

| Duration | Activities | Instructor | Resources |
|:---------|:-----------|:-----------|:---------:|
| 1.30pm BST<br>(75&nbsp;minutes) | Introduction to modular surrogate explainers. | Kacper&nbsp;Sokol | [**slides**]({{ "bias_surrogates.pdf" | prepend: page.slides }}) <br> [**demonstration**]({{ page.demo }}) |
| 2.45pm BST<br>(45&nbsp;minutes) | Hands-on with parameterising tabular surrogate explainers. | Kacper&nbsp;Sokol | **[Jupyter<br>Notebooks]({{ page.notebooks }})** |

## Instructors ##

### Kacper Sokol ###
Kacper is a research associate with the TAILOR project at
the University of Bristol.
His main research focus is transparency -- interpretability and
explainability -- of machine learning systems.
In particular, he has done work on enhancing transparency of logical predictive
models (and their ensembles) with counterfactual explanations.
Kacper is the designer and lead developer of the
[`FAT Forensics`](https://fat-forensics.org/) package.

<dl>
  <dt>Contact</dt>
  <dd><a href="mailto: K.Sokol@bristol.ac.uk">K.Sokol@bristol.ac.uk</a></dd>
</dl>

## References ##

[^1]: Kacper Sokol, Alexander Hepburn, Rafael Poyiadzi, Matthew Clifford,
      Raul Santos-Rodriguez, and Peter Flach. 2020. FAT Forensics: A Python
      Toolbox for Implementing and Deploying Fairness, Accountability and
      Transparency Algorithms in Predictive Systems. Journal of Open Source
      Software, 5(49), p.1904.
      <https://joss.theoj.org/papers/10.21105/joss.01904>

[^2]: Kacper Sokol, Alexander Hepburn, Raul Santos-Rodriguez, and
      Peter Flach. 2019. bLIMEy: Surrogate Prediction Explanations Beyond
      LIME. 2019 Workshop on Human-Centric Machine Learning (HCML 2019) at the
      33<sup>rd</sup> Conference on NeuralInformation Processing Systems
      (NeurIPS 2019), Vancouver, Canada (2019).
      <https://arxiv.org/abs/1910.13016>

[^3]: Kacper Sokol, Raul Santos-Rodriguez, and Peter Flach. 2019. FAT
      Forensics: A Python Toolbox for Algorithmic Fairness, Accountability and
      Transparency. arXiv preprint arXiv:1909.05167.
      <https://arxiv.org/abs/1909.05167>
