# Where Does the Understanding Come From When Explaining Automated Decision-making Systems? #

A myriad of approaches exists to help us peer inside automated decision-making systems
based on artificial intelligence and machine learning algorithms.
These tools and their insights, however, are socio-technological constructs themselves,
hence subject to human biases and preferences as well as technical limitations.
Under these conditions, how can we ensure that explanations are meaningful and fulfil
their role by leading to understanding?
In this talk I will demonstrate how different configurations of an explainability
algorithm may impact the resulting insights and show the importance of the strategy
employed to present them to the user, arguing in favour of a clear separation between
the technical and social aspects of such tools.

> Interactive slides for a [talk][talk] given at the
> [*AI and Humanity* cluster workshop][workshop] held at the Simons Institute
> for the Theory of Computing, UC Berkeley.
> The talk recording is available on [YouTube][youtube].

This directory holds the [presentation](slides) used for the presentation.
See <https://events.fat-forensics.org/2022_simons-institute/> for more details.

[talk]: https://simons.berkeley.edu/talks/tbd-453
[workshop]: https://simons.berkeley.edu/workshops/schedule/21107
[youtube]: https://youtu.be/9z-9yngCcTA
