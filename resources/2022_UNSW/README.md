# Never Let the Truth Get in the Way of a Good Story: The Importance of Multilevel Human Understanding in Explainable Artificial Intelligence #

Myriad approaches exist to help humans peer inside automated decision-making
systems based on artificial intelligence and machine learning algorithms.
These tools and the insights they produce, however, tend to be complex
socio-technological constructs themselves, hence subject to technical
limitations as well as human biases and (possibly ill-defined) preferences.
Under these conditions, how can we ensure that explanations are scientifically
sound, technically correct and socially meaningful,
therefore fulfil their role of leading to understanding?

In this talk I will provide a high-level introduction to and
overview of (key concepts in) explainable AI and interpretable ML,
followed by a deep dive into practical aspects of a popular
transparency algorithm.
Specifically, I will discuss the XAI and IML taxonomy captured by
*Explainability Fact Sheets*;
my attempt to define *explainability* in artificial intelligence and
machine learning; and
a preliminary framework intended to unify evaluation of
explainability approaches on a conceptual level.
Additionally, I will demonstrate how different configurations of an explainer
– that is often presented as a monolithic, end-to-end tool – may adversely
impact the resulting insights, using the example of a surrogate explainer.
I will then show the importance of the strategy employed to present
these pieces of information to a user, arguing in favour of a clear separation
between the technical and social aspects of explainability techniques.
Importantly, understanding these dependencies can help us to build bespoke
explainers that are robust, reliable, trustworthy and suitable for the unique
challenge at hand.

> Interactive slides for an invited talk given at the
> *University of New South Wales* (UNSW), Sydney.

This directory holds [slides](slides) used for the presentation.
See <https://events.fat-forensics.org/2022_unsw> for more details.
